{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2: Extract and clean the data from the data sources.\n",
    "##### Your first mission will be to extract all the data from the multitude of data sources, clean it, and then store it in a database that you will create. NOTE: The data is artificial so some columns might not give realistic values, for instance the latitude, longitude and card numbers columns. The rows you will need to clean will have clear errors; think missing values or characters that shouldn't be there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "\n",
    "aws_connector = DatabaseConnector(\n",
    "    creds_file=\"aws_db_creds.yaml\"\n",
    ")  # Initialize AWS connector for extraction\n",
    "data_extractor = DataExtractor()\n",
    "data_cleaner = DataCleaning()\n",
    "local_connector = DatabaseConnector(\n",
    "    creds_file=\"local_db_creds.yaml\"\n",
    ")  # Initialise local connector for uploading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of table names: ['legacy_store_details', 'dim_card_details', 'legacy_users', 'orders_table']\n",
      "Initial number of rows: 15320\n",
      "Rows after replacing 'NULL' strings: 15320\n",
      "Rows after dropping NULLs: 15299\n",
      "Rows after converting 'join_date': 15299\n",
      "Final rows after date cleaning in user data: 15284\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15284 entries, 0 to 15319\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   index          15284 non-null  int64         \n",
      " 1   first_name     15284 non-null  object        \n",
      " 2   last_name      15284 non-null  object        \n",
      " 3   date_of_birth  15284 non-null  object        \n",
      " 4   company        15284 non-null  object        \n",
      " 5   email_address  15284 non-null  object        \n",
      " 6   address        15284 non-null  object        \n",
      " 7   country        15284 non-null  object        \n",
      " 8   country_code   15284 non-null  object        \n",
      " 9   phone_number   15284 non-null  object        \n",
      " 10  join_date      15284 non-null  datetime64[ns]\n",
      " 11  user_uuid      15284 non-null  object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(10)\n",
      "memory usage: 1.5+ MB\n",
      "None\n",
      "Uploaded cleaned user data to the local sales_data database.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract data from AWS RDS\n",
    "table_names_list = aws_connector.list_db_tables()  # Find table name\n",
    "print(\"List of table names:\", table_names_list)\n",
    "# Output: List of table names: ['legacy_store_details', 'dim_card_details', 'legacy_users', 'orders_table']\n",
    "user_table_name = table_names_list[2]\n",
    "user_df = data_extractor.read_rds_table(aws_connector, user_table_name)\n",
    "\n",
    "# Step 2: Clean data\n",
    "cleaned_user_df = data_cleaner.clean_user_data(user_df)\n",
    "print(cleaned_user_df.info())\n",
    "\n",
    "# Step 3: Use local connector for uploading\n",
    "local_connector.upload_to_db(cleaned_user_df, \"dim_users\")\n",
    "print(\"Uploaded cleaned user data to the local sales_data database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import jpype dependencies. Fallback to subprocess.\n",
      "No module named 'technology'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of rows: 15309\n",
      "Rows after replacing 'NULL' strings: 15309\n",
      "Rows after dropping NULLs: 15298\n",
      "Rows after removing duplicates: 15298\n",
      "Rows with modified card numbers exported to debug_csv/task_4/modified_card_numbers.csv\n",
      "Rows after cleaning card numbers: 15297\n",
      "Final rows after date cleaning: 15284\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15284 entries, 0 to 15308\n",
      "Data columns (total 4 columns):\n",
      " #   Column                  Non-Null Count  Dtype         \n",
      "---  ------                  --------------  -----         \n",
      " 0   card_number             15284 non-null  object        \n",
      " 1   expiry_date             15284 non-null  object        \n",
      " 2   card_provider           15284 non-null  object        \n",
      " 3   date_payment_confirmed  15284 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(3)\n",
      "memory usage: 597.0+ KB\n",
      "None\n",
      "Uploaded 15284 rows to dim_card_details.\n",
      "Uploaded cleaned card data to the local sales_data database.\n"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "\n",
    "data_extractor = DataExtractor()\n",
    "data_cleaner = DataCleaning()\n",
    "local_connector = DatabaseConnector(\n",
    "    creds_file=\"local_db_creds.yaml\"\n",
    ")  # Initialise local connector for uploading\n",
    "pdf_link = \"https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf\"\n",
    "card_df = data_extractor.retrieve_pdf_data(pdf_link)\n",
    "# print(\"Card data extracted from PDF:\")\n",
    "# print(card_df.info()\n",
    "\n",
    "# Step 2: Clean card data\n",
    "cleaned_card_df = data_cleaner.clean_card_data(card_df)\n",
    "print(cleaned_card_df.info())\n",
    "# Step 3: Initialize local connector for uploading\n",
    "local_connector.upload_to_db(cleaned_card_df, \"dim_card_details\")\n",
    "print(\"Uploaded cleaned card data to the local sales_data database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stores to retrieve: 451\n",
      "Retrieved 451 stores from the API.\n",
      "Initial rows: 451\n",
      "Rows after replacing invalid strings with pd.NA: 451\n",
      "Rows after dropping NULLs: 447\n",
      "Rows after staff number cleanup: 447\n",
      "Rows after converting opening_date column into a datetime data type: 440\n",
      "Store 0 reintegrated after cleaning.\n",
      "Final rows after date validation: 441\n",
      "Cleaned store data has 441 rows.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 441 entries, 0 to 440\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   index          441 non-null    int64 \n",
      " 1   address        440 non-null    object\n",
      " 2   longitude      440 non-null    object\n",
      " 3   lat            0 non-null      object\n",
      " 4   locality       440 non-null    object\n",
      " 5   store_code     441 non-null    object\n",
      " 6   staff_numbers  441 non-null    object\n",
      " 7   opening_date   441 non-null    object\n",
      " 8   store_type     441 non-null    object\n",
      " 9   latitude       440 non-null    object\n",
      " 10  country_code   441 non-null    object\n",
      " 11  continent      441 non-null    object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 41.5+ KB\n",
      "None\n",
      "Uploaded cleaned store data to the local sales_data database.\n"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "import logging\n",
    "\n",
    "try:\n",
    "    # Initialize classes\n",
    "    data_extractor = DataExtractor()\n",
    "    data_cleaner = DataCleaning()\n",
    "    local_connector = DatabaseConnector(creds_file=\"local_db_creds.yaml\")\n",
    "\n",
    "    # Step 1: Retrieve the number of stores\n",
    "    number_of_stores_endpoint = (\n",
    "        \"https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores\"\n",
    "    )\n",
    "    num_stores = data_extractor.list_number_of_stores(number_of_stores_endpoint)\n",
    "    print(f\"Number of stores to retrieve: {num_stores}\")\n",
    "\n",
    "    # Step 2: Retrieve all stores data\n",
    "    retrieve_store_endpoint = (\n",
    "        \"https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details\"\n",
    "    )\n",
    "    stores_df = data_extractor.retrieve_stores_data(\n",
    "        retrieve_store_endpoint, num_stores\n",
    "    )\n",
    "    print(f\"Retrieved {len(stores_df)} stores from the API.\")\n",
    "\n",
    "    # Step 3: Clean the store data\n",
    "    cleaned_stores_df = data_cleaner.clean_store_data(stores_df)\n",
    "    print(f\"Cleaned store data has {len(cleaned_stores_df)} rows.\")\n",
    "    print(cleaned_stores_df.info())\n",
    "\n",
    "    # Step 4: Upload the cleaned data to the database\n",
    "    local_connector.upload_to_db(cleaned_stores_df, \"dim_store_details\")\n",
    "    print(\"Uploaded cleaned store data to the local sales_data database.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error in main workflow: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rows: 1853\n",
      "Rows after replacing invalid strings: 1853\n",
      "Row 1779: Set weight to 0 (previously NULL) as all the other values are valid.\n",
      "Rows after dropping NULLs: 1846\n",
      "Rows after converting weights to kg: 1846\n",
      "Final rows after cleaning: 1846\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1846 entries, 0 to 1852\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Unnamed: 0     1846 non-null   int64 \n",
      " 1   product_name   1846 non-null   object\n",
      " 2   product_price  1846 non-null   object\n",
      " 3   weight         1846 non-null   object\n",
      " 4   category       1846 non-null   object\n",
      " 5   EAN            1846 non-null   object\n",
      " 6   date_added     1846 non-null   object\n",
      " 7   uuid           1846 non-null   object\n",
      " 8   removed        1846 non-null   object\n",
      " 9   product_code   1846 non-null   object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 158.6+ KB\n",
      "None\n",
      "Uploaded cleaned products data to the local sales_data database.\n"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "\n",
    "# Step 1: Extract data from S3\n",
    "data_extractor = DataExtractor()\n",
    "s3_address = \"s3://data-handling-public/products.csv\"\n",
    "products_df = data_extractor.extract_from_s3(s3_address)\n",
    "\n",
    "# Step 2: Convert product weights unit to kg\n",
    "data_cleaner = DataCleaning()\n",
    "products_df = data_cleaner.convert_product_weights(products_df)\n",
    "\n",
    "# Step 3: Clean products data\n",
    "products_df = data_cleaner.clean_products_data(products_df)\n",
    "print(products_df.info())\n",
    "\n",
    "# Step 4: Upload to database\n",
    "local_connector = DatabaseConnector(creds_file=\"local_db_creds.yaml\")\n",
    "local_connector.upload_to_db(products_df, \"dim_products\")\n",
    "\n",
    "print(\"Uploaded cleaned products data to the local sales_data database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: List All Tables in the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of table names: ['legacy_store_details', 'dim_card_details', 'legacy_users', 'orders_table']\n"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "\n",
    "# Initialize the AWS RDS connector\n",
    "aws_connector = DatabaseConnector(creds_file=\"aws_db_creds.yaml\")\n",
    "\n",
    "# List all tables in the database\n",
    "table_names = aws_connector.list_db_tables()\n",
    "print(\"List of table names:\", table_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extract Orders Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders data extracted successfully.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120123 entries, 0 to 120122\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   level_0           120123 non-null  int64  \n",
      " 1   index             120123 non-null  int64  \n",
      " 2   date_uuid         120123 non-null  object \n",
      " 3   first_name        15284 non-null   object \n",
      " 4   last_name         15284 non-null   object \n",
      " 5   user_uuid         120123 non-null  object \n",
      " 6   card_number       120123 non-null  int64  \n",
      " 7   store_code        120123 non-null  object \n",
      " 8   product_code      120123 non-null  object \n",
      " 9   1                 0 non-null       float64\n",
      " 10  product_quantity  120123 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(6)\n",
      "memory usage: 10.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from data_extraction import DataExtractor\n",
    "\n",
    "# Initialize the data extractor\n",
    "data_extractor = DataExtractor()\n",
    "\n",
    "# Extract the orders data\n",
    "orders_df = data_extractor.read_rds_table(aws_connector, \"orders_table\")\n",
    "print(\"Orders data extracted successfully.\")\n",
    "print(orders_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Clean Orders Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Upload Cleaned Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning: 120123\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120123 entries, 0 to 120122\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   level_0           120123 non-null  int64 \n",
      " 1   index             120123 non-null  int64 \n",
      " 2   date_uuid         120123 non-null  object\n",
      " 3   user_uuid         120123 non-null  object\n",
      " 4   card_number       120123 non-null  int64 \n",
      " 5   store_code        120123 non-null  object\n",
      " 6   product_code      120123 non-null  object\n",
      " 7   product_quantity  120123 non-null  int64 \n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 7.3+ MB\n",
      "None\n",
      "Uploaded 120123 rows to orders_table.\n",
      "Uploaded cleaned orders data to the local sales_data database.\n"
     ]
    }
   ],
   "source": [
    "from data_cleaning import DataCleaning\n",
    "\n",
    "# Initialize the data cleaner\n",
    "data_cleaner = DataCleaning()\n",
    "\n",
    "# Clean the orders data\n",
    "cleaned_orders_df = data_cleaner.clean_orders_data(orders_df)\n",
    "print(cleaned_orders_df.info())\n",
    "\n",
    "# Initialize the local database connector\n",
    "local_connector = DatabaseConnector(creds_file=\"local_db_creds.yaml\")\n",
    "\n",
    "# Upload the cleaned data to the local database\n",
    "local_connector.upload_to_db(cleaned_orders_df, \"orders_table\")\n",
    "print(\"Uploaded cleaned orders data to the local sales_data database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date times data extracted successfully.\n",
      "Rows after cleaning: 120123\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 120123 entries, 0 to 120160\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   timestamp    120123 non-null  object\n",
      " 1   month        120123 non-null  int64 \n",
      " 2   year         120123 non-null  int64 \n",
      " 3   day          120123 non-null  int64 \n",
      " 4   time_period  120123 non-null  object\n",
      " 5   date_uuid    120123 non-null  object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 6.4+ MB\n",
      "None\n",
      "Uploaded 120123 rows to dim_date_times.\n",
      "Uploaded cleaned date times data to the local sales_data database.\n"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "\n",
    "# Step 1: Extract JSON data from S3\n",
    "data_extractor = DataExtractor()\n",
    "s3_url = \"https://data-handling-public.s3.eu-west-1.amazonaws.com/date_details.json\"\n",
    "date_times_df = data_extractor.extract_json_from_s3(s3_url)\n",
    "\n",
    "if not date_times_df.empty:\n",
    "\n",
    "    # Step 2: Clean the date times data\n",
    "    data_cleaner = DataCleaning()\n",
    "    cleaned_date_times_df = data_cleaner.clean_date_times_data(date_times_df)\n",
    "    print(date_times_df.info())\n",
    "\n",
    "    # Step 3: Upload cleaned data to the local database\n",
    "    local_connector = DatabaseConnector(creds_file=\"local_db_creds.yaml\")\n",
    "    local_connector.upload_to_db(cleaned_date_times_df, \"dim_date_times\")\n",
    "    print(\"Uploaded cleaned date times data to the local sales_data database.\")\n",
    "else:\n",
    "    print(\"Failed to extract date times data from S3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3: Create the database schema\n",
    "##### Develop the star-based schema of the database, ensuring that the columns are of the correct data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning: 120123\n",
      "Uploaded 120123 rows to orders_table.\n"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "from sqlalchemy import UUID, VARCHAR, SMALLINT\n",
    "\n",
    "# Step 1: Extract orders data\n",
    "data_extractor = DataExtractor()\n",
    "aws_connector = DatabaseConnector(creds_file=\"aws_db_creds.yaml\")\n",
    "orders_df = data_extractor.read_rds_table(aws_connector, \"orders_table\")\n",
    "\n",
    "# Step 2: Clean orders data\n",
    "data_cleaner = DataCleaning()\n",
    "cleaned_orders_df = data_cleaner.clean_orders_data(orders_df)\n",
    "\n",
    "# Calculate maximum lengths\n",
    "max_card_number_length = cleaned_orders_df[\"card_number\"].astype(str).str.len().max()\n",
    "max_store_code_length = cleaned_orders_df[\"store_code\"].astype(str).str.len().max()\n",
    "max_product_code_length = cleaned_orders_df[\"product_code\"].astype(str).str.len().max()\n",
    "\n",
    "# print(f\"Max card_number length: {max_card_number_length}\")\n",
    "# print(f\"Max store_code length: {max_store_code_length}\")\n",
    "# print(f\"Max product_code length: {max_product_code_length}\")\n",
    "\n",
    "# Step 3: Define correct data types\n",
    "data_types = {\n",
    "    \"date_uuid\": UUID,\n",
    "    \"user_uuid\": UUID,\n",
    "    \"card_number\": VARCHAR(max_card_number_length),\n",
    "    \"store_code\": VARCHAR(max_store_code_length),\n",
    "    \"product_code\": VARCHAR(max_product_code_length),\n",
    "    \"product_quantity\": SMALLINT,\n",
    "}\n",
    "\n",
    "# Step 4: Upload to database with correct data types\n",
    "local_connector = DatabaseConnector(creds_file=\"local_db_creds.yaml\")\n",
    "local_connector.upload_to_db(cleaned_orders_df, \"orders_table\", dtype=data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run following SQL Query to Check Data Types in pgAdmin 4\n",
    "# SELECT column_name, data_type\n",
    "# FROM information_schema.columns\n",
    "# WHERE table_name = 'orders_table';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of rows: 15284\n",
      "Rows after replacing 'NULL' strings: 15284\n",
      "Rows after dropping NULLs: 15284\n",
      "Rows after converting 'join_date': 15284\n",
      "Final rows after date cleaning in user data: 15284\n",
      "Max country_code length: 3\n",
      "Max first_name length: 14\n",
      "Max last_name length: 15\n",
      "Uploaded 15284 rows to dim_users.\n"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "from sqlalchemy import UUID, VARCHAR, DATE\n",
    "\n",
    "# Step 1: Extract users data\n",
    "data_extractor = DataExtractor()\n",
    "local_connector = DatabaseConnector(creds_file=\"local_db_creds.yaml\")\n",
    "dim_users_df = data_extractor.read_rds_table(local_connector, \"dim_users\")\n",
    "\n",
    "# Step 2: Clean users data (if necessary)\n",
    "data_cleaner = DataCleaning()\n",
    "cleaned_dim_users_df = data_cleaner.clean_user_data(dim_users_df)\n",
    "\n",
    "# Calculate maximum length of country_code\n",
    "max_country_code_length = dim_users_df[\"country_code\"].astype(str).str.len().max()\n",
    "max_first_name_length = dim_users_df[\"first_name\"].astype(str).str.len().max()\n",
    "max_last_name_length = dim_users_df[\"last_name\"].astype(str).str.len().max()\n",
    "print(f\"Max country_code length: {max_country_code_length}\")\n",
    "print(f\"Max first_name length: {max_first_name_length}\")\n",
    "print(f\"Max last_name length: {max_last_name_length}\")\n",
    "\n",
    "# Step 3: Define correct data types\n",
    "data_types = {\n",
    "    \"first_name\": VARCHAR(max_first_name_length),\n",
    "    \"last_name\": VARCHAR(max_last_name_length),\n",
    "    \"date_of_birth\": DATE,\n",
    "    \"country_code\": VARCHAR(max_country_code_length),\n",
    "    \"user_uuid\": UUID,\n",
    "    \"join_date\": DATE,\n",
    "}\n",
    "\n",
    "# Step 4: Upload to database with correct data types\n",
    "local_connector = DatabaseConnector(creds_file=\"local_db_creds.yaml\")\n",
    "local_connector.upload_to_db(cleaned_dim_users_df, \"dim_users\", dtype=data_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rows: 441\n",
      "Rows after replacing invalid strings with pd.NA: 441\n",
      "Rows after dropping NULLs: 440\n",
      "Rows after staff number cleanup: 440\n",
      "Rows after converting opening_date column into a datetime data type: 440\n",
      "Store 0 reintegrated after cleaning.\n",
      "Final rows after date validation: 441\n",
      "Max store_code length: 12\n",
      "Max country_code length: 2\n",
      "Max locality length: 20\n",
      "Max continent length: 9\n",
      "Max store_type length: 11\n",
      "Uploaded 441 rows to dim_store_details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/9qrkg95j2_9c9cd9zflhxzc40000gn/T/ipykernel_34033/3986454532.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  cleaned_dim_store_details_df[\"locality\"].replace(\"N/A\", pd.NA, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "from sqlalchemy import VARCHAR, NUMERIC, SMALLINT, DATE\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Extract store details data\n",
    "data_extractor = DataExtractor()\n",
    "local_connector = DatabaseConnector(creds_file=\"local_db_creds.yaml\")\n",
    "dim_store_details_df = data_extractor.read_rds_table(\n",
    "    local_connector, \"dim_store_details\"\n",
    ")\n",
    "\n",
    "# Step 2: Clean store details data\n",
    "data_cleaner = DataCleaning()\n",
    "cleaned_dim_store_details_df = data_cleaner.clean_store_data(dim_store_details_df)\n",
    "\n",
    "# Step 3: Merge latitude columns\n",
    "cleaned_dim_store_details_df[\"latitude\"] = cleaned_dim_store_details_df[\n",
    "    \"latitude\"\n",
    "].combine_first(cleaned_dim_store_details_df[\"lat\"])\n",
    "cleaned_dim_store_details_df.drop(\"lat\", axis=1, inplace=True)\n",
    "\n",
    "# Replace \"N/A\" with NULL in the 'location' column\n",
    "cleaned_dim_store_details_df[\"locality\"].replace(\"N/A\", pd.NA, inplace=True)\n",
    "\n",
    "# Step 4: Calculate maximum lengths for VARCHAR columns\n",
    "max_store_code_length = (\n",
    "    cleaned_dim_store_details_df[\"store_code\"].astype(str).str.len().max()\n",
    ")\n",
    "max_country_code_length = (\n",
    "    cleaned_dim_store_details_df[\"country_code\"].astype(str).str.len().max()\n",
    ")\n",
    "max_locality_length = (\n",
    "    cleaned_dim_store_details_df[\"locality\"].astype(str).str.len().max()\n",
    ")\n",
    "max_continent_length = (\n",
    "    cleaned_dim_store_details_df[\"continent\"].astype(str).str.len().max()\n",
    ")\n",
    "max_store_type_length = (\n",
    "    cleaned_dim_store_details_df[\"store_type\"].astype(str).str.len().max()\n",
    ")\n",
    "\n",
    "print(f\"Max store_code length: {max_store_code_length}\")\n",
    "print(f\"Max country_code length: {max_country_code_length}\")\n",
    "print(f\"Max locality length: {max_locality_length}\")\n",
    "print(f\"Max continent length: {max_continent_length}\")\n",
    "print(f\"Max store_type length: {max_store_type_length}\")\n",
    "\n",
    "# Step 5: Define correct data types\n",
    "data_types = {\n",
    "    \"longitude\": NUMERIC,\n",
    "    \"locality\": VARCHAR(max_locality_length),\n",
    "    \"store_code\": VARCHAR(max_store_code_length),\n",
    "    \"staff_numbers\": SMALLINT,\n",
    "    \"opening_date\": DATE,\n",
    "    \"store_type\": VARCHAR(max_store_type_length),\n",
    "    \"latitude\": NUMERIC,\n",
    "    \"country_code\": VARCHAR(max_country_code_length),\n",
    "    \"continent\": VARCHAR(max_continent_length),\n",
    "}\n",
    "\n",
    "# Step 6: Upload to database with correct data types\n",
    "local_connector.upload_to_db(\n",
    "    cleaned_dim_store_details_df, \"dim_store_details\", dtype=data_types\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (Refer to sales_data_milstone3.sql for the SQL code)\n",
    "##### The following code for Task 4 hasn't been tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "from sqlalchemy import VARCHAR, NUMERIC\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Extract products data\n",
    "data_extractor = DataExtractor()\n",
    "local_connector = DatabaseConnector(creds_file=\"local_db_creds.yaml\")\n",
    "dim_products_df = data_extractor.read_rds_table(local_connector, \"dim_products\")\n",
    "\n",
    "# Step 2: Clean products data\n",
    "data_cleaner = DataCleaning()\n",
    "cleaned_dim_products_df = data_cleaner.clean_products_data(dim_products_df)\n",
    "\n",
    "# Step 3: Remove the '£' character from the 'product_price' column\n",
    "cleaned_dim_products_df[\"product_price\"] = cleaned_dim_products_df[\n",
    "    \"product_price\"\n",
    "].str.replace(\"£\", \"\")\n",
    "\n",
    "# Step 4: Convert the 'weight' column to numeric\n",
    "cleaned_dim_products_df[\"weight\"] = pd.to_numeric(\n",
    "    cleaned_dim_products_df[\"weight\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Step 5: Add the 'weight_class' column\n",
    "cleaned_dim_products_df[\"weight_class\"] = pd.cut(\n",
    "    cleaned_dim_products_df[\"weight\"],\n",
    "    bins=[-1, 2, 40, 140, float(\"inf\")],\n",
    "    labels=[\"Light\", \"Mid_Sized\", \"Heavy\", \"Truck_Required\"],\n",
    ")\n",
    "\n",
    "# Step 6: Calculate maximum lengths for VARCHAR columns\n",
    "max_product_code_length = (\n",
    "    cleaned_dim_products_df[\"product_code\"].astype(str).str.len().max()\n",
    ")\n",
    "max_weight_class_length = (\n",
    "    cleaned_dim_products_df[\"weight_class\"].astype(str).str.len().max()\n",
    ")\n",
    "max_product_price_length = (\n",
    "    cleaned_dim_products_df[\"product_price\"].astype(str).str.len().max()\n",
    ")\n",
    "\n",
    "print(f\"Max product_code length: {max_product_code_length}\")\n",
    "print(f\"Max weight_class length: {max_weight_class_length}\")\n",
    "print(f\"Max product_price length: {max_product_price_length}\")\n",
    "\n",
    "# Step 7: Define correct data types\n",
    "data_types = {\n",
    "    \"product_code\": VARCHAR(max_product_code_length),\n",
    "    \"product_price\": NUMERIC,\n",
    "    \"weight\": NUMERIC,\n",
    "    \"weight_class\": VARCHAR(max_weight_class_length),\n",
    "}\n",
    "\n",
    "# Step 8: Upload to database with correct data types\n",
    "local_connector.upload_to_db(cleaned_dim_products_df, \"dim_products\", dtype=data_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rows: 1846\n",
      "Rows after replacing invalid strings: 1846\n",
      "Rows after dropping NULLs: 1846\n",
      "Rows after converting weights to kg: 1846\n",
      "Final rows after cleaning: 1846\n",
      "Max EAN length: 17\n",
      "Max product_code length: 11\n",
      "Max weight_class length: 14\n",
      "Uploaded 1846 rows to dim_products.\n"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "from sqlalchemy import VARCHAR, NUMERIC, DATE, UUID, BOOLEAN\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Extract products data\n",
    "data_extractor = DataExtractor()\n",
    "local_connector = DatabaseConnector(creds_file=\"local_db_creds.yaml\")\n",
    "dim_products_df = data_extractor.read_rds_table(local_connector, \"dim_products\")\n",
    "\n",
    "# Step 2: Clean products data\n",
    "data_cleaner = DataCleaning()\n",
    "cleaned_dim_products_df = data_cleaner.clean_products_data(dim_products_df)\n",
    "\n",
    "# Step 3: Rename the 'removed' column to 'still_available'\n",
    "cleaned_dim_products_df.rename(columns={\"removed\": \"still_available\"}, inplace=True)\n",
    "\n",
    "# Step 4: Update 'still_available' values\n",
    "cleaned_dim_products_df[\"still_available\"] = cleaned_dim_products_df[\n",
    "    \"still_available\"\n",
    "].map(\n",
    "    {\n",
    "        \"Still_avaliable\": True,\n",
    "        \"Removed\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step 5: Calculate maximum lengths for VARCHAR columns\n",
    "max_ean_length = cleaned_dim_products_df[\"EAN\"].astype(str).str.len().max()\n",
    "max_product_code_length = (\n",
    "    cleaned_dim_products_df[\"product_code\"].astype(str).str.len().max()\n",
    ")\n",
    "max_weight_class_length = (\n",
    "    cleaned_dim_products_df[\"weight_class\"].astype(str).str.len().max()\n",
    ")\n",
    "\n",
    "print(f\"Max EAN length: {max_ean_length}\")\n",
    "print(f\"Max product_code length: {max_product_code_length}\")\n",
    "print(f\"Max weight_class length: {max_weight_class_length}\")\n",
    "\n",
    "# Step 6: Define correct data types\n",
    "data_types = {\n",
    "    \"product_price\": NUMERIC,\n",
    "    \"weight\": NUMERIC,\n",
    "    \"EAN\": VARCHAR(max_ean_length),\n",
    "    \"product_code\": VARCHAR(max_product_code_length),\n",
    "    \"date_added\": DATE,\n",
    "    \"uuid\": UUID,\n",
    "    \"still_available\": BOOLEAN,\n",
    "    \"weight_class\": VARCHAR(max_weight_class_length),\n",
    "}\n",
    "\n",
    "# Step 7: Upload to database with correct data types\n",
    "local_connector.upload_to_db(cleaned_dim_products_df, \"dim_products\", dtype=data_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning: 120123\n",
      "Max month length: 2\n",
      "Max year length: 4\n",
      "Max day length: 2\n",
      "Max time_period length: 10\n",
      "Uploaded 120123 rows to dim_date_times.\n"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "from sqlalchemy import VARCHAR, UUID\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Extract date times data\n",
    "data_extractor = DataExtractor()\n",
    "local_connector = DatabaseConnector(creds_file=\"local_db_creds.yaml\")\n",
    "dim_date_times_df = data_extractor.read_rds_table(local_connector, \"dim_date_times\")\n",
    "\n",
    "# Step 2: Clean date times data\n",
    "data_cleaner = DataCleaning()\n",
    "cleaned_dim_date_times_df = data_cleaner.clean_date_times_data(dim_date_times_df)\n",
    "\n",
    "# Step 3: Calculate maximum lengths for VARCHAR columns\n",
    "max_month_length = cleaned_dim_date_times_df[\"month\"].astype(str).str.len().max()\n",
    "max_year_length = cleaned_dim_date_times_df[\"year\"].astype(str).str.len().max()\n",
    "max_day_length = cleaned_dim_date_times_df[\"day\"].astype(str).str.len().max()\n",
    "max_time_period_length = (\n",
    "    cleaned_dim_date_times_df[\"time_period\"].astype(str).str.len().max()\n",
    ")\n",
    "\n",
    "print(f\"Max month length: {max_month_length}\")\n",
    "print(f\"Max year length: {max_year_length}\")\n",
    "print(f\"Max day length: {max_day_length}\")\n",
    "print(f\"Max time_period length: {max_time_period_length}\")\n",
    "\n",
    "# Step 4: Define correct data types\n",
    "data_types = {\n",
    "    \"month\": VARCHAR(max_month_length),\n",
    "    \"year\": VARCHAR(max_year_length),\n",
    "    \"day\": VARCHAR(max_day_length),\n",
    "    \"time_period\": VARCHAR(max_time_period_length),\n",
    "    \"date_uuid\": UUID,\n",
    "}\n",
    "\n",
    "# Step 5: Upload to database with correct data types\n",
    "local_connector.upload_to_db(\n",
    "    cleaned_dim_date_times_df, \"dim_date_times\", dtype=data_types\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of rows: 15284\n",
      "Rows after replacing 'NULL' strings: 15284\n",
      "Rows after dropping NULLs: 15284\n",
      "Rows after removing duplicates: 15284\n",
      "Rows with modified card numbers exported to debug_csv/task_4/modified_card_numbers.csv\n",
      "Rows after cleaning card numbers: 15284\n",
      "Final rows after date cleaning: 15284\n",
      "Max card_number length: 19\n",
      "Max expiry_date length: 5\n",
      "Uploaded 15284 rows to dim_card_details.\n"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "from sqlalchemy import VARCHAR, DATE\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Extract card details data\n",
    "data_extractor = DataExtractor()\n",
    "local_connector = DatabaseConnector(creds_file=\"local_db_creds.yaml\")\n",
    "dim_card_details_df = data_extractor.read_rds_table(local_connector, \"dim_card_details\")\n",
    "\n",
    "# Step 2: Clean card details data\n",
    "data_cleaner = DataCleaning()\n",
    "cleaned_dim_card_details_df = data_cleaner.clean_card_data(dim_card_details_df)\n",
    "\n",
    "# Step 3: Calculate maximum lengths for VARCHAR columns\n",
    "max_card_number_length = (\n",
    "    cleaned_dim_card_details_df[\"card_number\"].astype(str).str.len().max()\n",
    ")\n",
    "max_expiry_date_length = (\n",
    "    cleaned_dim_card_details_df[\"expiry_date\"].astype(str).str.len().max()\n",
    ")\n",
    "\n",
    "print(f\"Max card_number length: {max_card_number_length}\")\n",
    "print(f\"Max expiry_date length: {max_expiry_date_length}\")\n",
    "\n",
    "# Step 4: Define correct data types\n",
    "data_types = {\n",
    "    \"card_number\": VARCHAR(max_card_number_length),\n",
    "    \"expiry_date\": VARCHAR(max_expiry_date_length),\n",
    "    \"date_payment_confirmed\": DATE,\n",
    "}\n",
    "\n",
    "# Step 5: Upload to database with correct data types\n",
    "local_connector.upload_to_db(\n",
    "    cleaned_dim_card_details_df, \"dim_card_details\", dtype=data_types\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8 (Refer to sales_data_milstone3.sql for the SQL code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9 (Refer to sales_data_milstone3.sql for the SQL code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
