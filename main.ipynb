{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "\n",
    "aws_connector = DatabaseConnector(\n",
    "    creds_file=\"aws_db_creds.yaml\"\n",
    ")  # Initialize AWS connector for extraction\n",
    "data_extractor = DataExtractor()\n",
    "data_cleaner = DataCleaning()\n",
    "local_connector = DatabaseConnector(\n",
    "    creds_file=\"local_db_creds.yaml\"\n",
    ")  # Initialise local connector for uploading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of table names: ['legacy_store_details', 'dim_card_details', 'legacy_users', 'orders_table']\n",
      "Initial number of rows: 15320\n",
      "Rows after replacing 'NULL' strings: 15320\n",
      "Rows after dropping NULLs: 15299\n",
      "Rows after converting 'join_date': 15299\n",
      "Final rows after date cleaning in user data: 15284\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15284 entries, 0 to 15319\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   index          15284 non-null  int64         \n",
      " 1   first_name     15284 non-null  object        \n",
      " 2   last_name      15284 non-null  object        \n",
      " 3   date_of_birth  15284 non-null  object        \n",
      " 4   company        15284 non-null  object        \n",
      " 5   email_address  15284 non-null  object        \n",
      " 6   address        15284 non-null  object        \n",
      " 7   country        15284 non-null  object        \n",
      " 8   country_code   15284 non-null  object        \n",
      " 9   phone_number   15284 non-null  object        \n",
      " 10  join_date      15284 non-null  datetime64[ns]\n",
      " 11  user_uuid      15284 non-null  object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(10)\n",
      "memory usage: 1.5+ MB\n",
      "None\n",
      "Uploaded cleaned user data to the local sales_data database.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract data from AWS RDS\n",
    "table_names_list = aws_connector.list_db_tables()  # Find table name\n",
    "print(\"List of table names:\", table_names_list)\n",
    "# Output: List of table names: ['legacy_store_details', 'dim_card_details', 'legacy_users', 'orders_table']\n",
    "user_table_name = table_names_list[2]\n",
    "user_df = data_extractor.read_rds_table(aws_connector, user_table_name)\n",
    "\n",
    "# Step 2: Clean data\n",
    "cleaned_user_df = data_cleaner.clean_user_data(user_df)\n",
    "print(cleaned_user_df.info())\n",
    "\n",
    "# Step 3: Use local connector for uploading\n",
    "local_connector.upload_to_db(cleaned_user_df, \"dim_users\")\n",
    "print(\"Uploaded cleaned user data to the local sales_data database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of rows: 15309\n",
      "Rows after replacing 'NULL' strings: 15309\n",
      "Rows after dropping NULLs: 15298\n",
      "Rows after removing duplicates: 15298\n",
      "Rows after removing non-numeric card numbers: 15284\n",
      "Final rows after date cleaning: 15284\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15284 entries, 0 to 15308\n",
      "Data columns (total 4 columns):\n",
      " #   Column                  Non-Null Count  Dtype         \n",
      "---  ------                  --------------  -----         \n",
      " 0   card_number             15284 non-null  object        \n",
      " 1   expiry_date             15284 non-null  object        \n",
      " 2   card_provider           15284 non-null  object        \n",
      " 3   date_payment_confirmed  15284 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(3)\n",
      "memory usage: 597.0+ KB\n",
      "None\n",
      "Uploaded cleaned card data to the local sales_data database.\n"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "\n",
    "data_extractor = DataExtractor()\n",
    "data_cleaner = DataCleaning()\n",
    "local_connector = DatabaseConnector(\n",
    "    creds_file=\"local_db_creds.yaml\"\n",
    ")  # Initialise local connector for uploading\n",
    "pdf_link = \"https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf\"\n",
    "card_df = data_extractor.retrieve_pdf_data(pdf_link)\n",
    "# print(\"Card data extracted from PDF:\")\n",
    "# print(card_df.info()\n",
    "\n",
    "# Step 2: Clean card data\n",
    "cleaned_card_df = data_cleaner.clean_card_data(card_df)\n",
    "print(cleaned_card_df.info())\n",
    "# Step 3: Initialize local connector for uploading\n",
    "local_connector.upload_to_db(cleaned_card_df, \"dim_card_details\")\n",
    "print(\"Uploaded cleaned card data to the local sales_data database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stores to retrieve: 451\n",
      "Retrieved 451 stores from the API.\n",
      "Initial rows: 451\n",
      "Rows after replacing invalid strings with pd.NA: 451\n",
      "Rows after dropping NULLs: 447\n",
      "Rows after staff number cleanup: 447\n",
      "Rows after converting opening_date column into a datetime data type: 440\n",
      "Store 0 reintegrated after cleaning.\n",
      "Final rows after date validation: 441\n",
      "Cleaned store data has 441 rows.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 441 entries, 0 to 440\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   index          441 non-null    int64 \n",
      " 1   address        440 non-null    object\n",
      " 2   longitude      440 non-null    object\n",
      " 3   lat            0 non-null      object\n",
      " 4   locality       440 non-null    object\n",
      " 5   store_code     441 non-null    object\n",
      " 6   staff_numbers  441 non-null    object\n",
      " 7   opening_date   441 non-null    object\n",
      " 8   store_type     441 non-null    object\n",
      " 9   latitude       440 non-null    object\n",
      " 10  country_code   441 non-null    object\n",
      " 11  continent      441 non-null    object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 41.5+ KB\n",
      "None\n",
      "Uploaded cleaned store data to the local sales_data database.\n"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "import logging\n",
    "\n",
    "try:\n",
    "    # Initialize classes\n",
    "    data_extractor = DataExtractor()\n",
    "    data_cleaner = DataCleaning()\n",
    "    local_connector = DatabaseConnector(creds_file=\"local_db_creds.yaml\")\n",
    "\n",
    "    # Step 1: Retrieve the number of stores\n",
    "    number_of_stores_endpoint = (\n",
    "        \"https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores\"\n",
    "    )\n",
    "    num_stores = data_extractor.list_number_of_stores(number_of_stores_endpoint)\n",
    "    print(f\"Number of stores to retrieve: {num_stores}\")\n",
    "\n",
    "    # Step 2: Retrieve all stores data\n",
    "    retrieve_store_endpoint = (\n",
    "        \"https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details\"\n",
    "    )\n",
    "    stores_df = data_extractor.retrieve_stores_data(\n",
    "        retrieve_store_endpoint, num_stores\n",
    "    )\n",
    "    print(f\"Retrieved {len(stores_df)} stores from the API.\")\n",
    "\n",
    "    # Step 3: Clean the store data\n",
    "    cleaned_stores_df = data_cleaner.clean_store_data(stores_df)\n",
    "    print(f\"Cleaned store data has {len(cleaned_stores_df)} rows.\")\n",
    "    print(cleaned_stores_df.info())\n",
    "\n",
    "    # Step 4: Upload the cleaned data to the database\n",
    "    local_connector.upload_to_db(cleaned_stores_df, \"dim_store_details\")\n",
    "    print(\"Uploaded cleaned store data to the local sales_data database.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error in main workflow: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rows: 1853\n",
      "Rows after replacing invalid strings: 1853\n",
      "Row 1779: Set weight to 0 (previously NULL) as all the other values are valid.\n",
      "Rows after dropping NULLs: 1846\n",
      "Rows after converting weights to kg: 1846\n",
      "Final rows after cleaning: 1846\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1846 entries, 0 to 1852\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Unnamed: 0     1846 non-null   int64 \n",
      " 1   product_name   1846 non-null   object\n",
      " 2   product_price  1846 non-null   object\n",
      " 3   weight         1846 non-null   object\n",
      " 4   category       1846 non-null   object\n",
      " 5   EAN            1846 non-null   object\n",
      " 6   date_added     1846 non-null   object\n",
      " 7   uuid           1846 non-null   object\n",
      " 8   removed        1846 non-null   object\n",
      " 9   product_code   1846 non-null   object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 158.6+ KB\n",
      "None\n",
      "Uploaded cleaned products data to the local sales_data database.\n"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "\n",
    "# Step 1: Extract data from S3\n",
    "data_extractor = DataExtractor()\n",
    "s3_address = \"s3://data-handling-public/products.csv\"\n",
    "products_df = data_extractor.extract_from_s3(s3_address)\n",
    "\n",
    "# Step 2: Convert product weights unit to kg\n",
    "data_cleaner = DataCleaning()\n",
    "products_df = data_cleaner.convert_product_weights(products_df)\n",
    "\n",
    "# Step 3: Clean products data\n",
    "products_df = data_cleaner.clean_products_data(products_df)\n",
    "print(products_df.info())\n",
    "\n",
    "# Step 4: Upload to database\n",
    "local_connector = DatabaseConnector(creds_file=\"local_db_creds.yaml\")\n",
    "local_connector.upload_to_db(products_df, \"dim_products\")\n",
    "\n",
    "print(\"Uploaded cleaned products data to the local sales_data database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: List All Tables in the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of table names: ['legacy_store_details', 'dim_card_details', 'legacy_users', 'orders_table']\n"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "\n",
    "# Initialize the AWS RDS connector\n",
    "aws_connector = DatabaseConnector(creds_file=\"aws_db_creds.yaml\")\n",
    "\n",
    "# List all tables in the database\n",
    "table_names = aws_connector.list_db_tables()\n",
    "print(\"List of table names:\", table_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extract Orders Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders data extracted successfully.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120123 entries, 0 to 120122\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   level_0           120123 non-null  int64  \n",
      " 1   index             120123 non-null  int64  \n",
      " 2   date_uuid         120123 non-null  object \n",
      " 3   first_name        15284 non-null   object \n",
      " 4   last_name         15284 non-null   object \n",
      " 5   user_uuid         120123 non-null  object \n",
      " 6   card_number       120123 non-null  int64  \n",
      " 7   store_code        120123 non-null  object \n",
      " 8   product_code      120123 non-null  object \n",
      " 9   1                 0 non-null       float64\n",
      " 10  product_quantity  120123 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(6)\n",
      "memory usage: 10.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from data_extraction import DataExtractor\n",
    "\n",
    "# Initialize the data extractor\n",
    "data_extractor = DataExtractor()\n",
    "\n",
    "# Extract the orders data\n",
    "orders_df = data_extractor.read_rds_table(aws_connector, \"orders_table\")\n",
    "print(\"Orders data extracted successfully.\")\n",
    "print(orders_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Clean Orders Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Upload Cleaned Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning: 120123\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120123 entries, 0 to 120122\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   level_0           120123 non-null  int64 \n",
      " 1   index             120123 non-null  int64 \n",
      " 2   date_uuid         120123 non-null  object\n",
      " 3   user_uuid         120123 non-null  object\n",
      " 4   card_number       120123 non-null  int64 \n",
      " 5   store_code        120123 non-null  object\n",
      " 6   product_code      120123 non-null  object\n",
      " 7   product_quantity  120123 non-null  int64 \n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 7.3+ MB\n",
      "None\n",
      "Uploaded cleaned orders data to the local sales_data database.\n"
     ]
    }
   ],
   "source": [
    "from data_cleaning import DataCleaning\n",
    "\n",
    "# Initialize the data cleaner\n",
    "data_cleaner = DataCleaning()\n",
    "\n",
    "# Clean the orders data\n",
    "cleaned_orders_df = data_cleaner.clean_orders_data(orders_df)\n",
    "print(cleaned_orders_df.info())\n",
    "\n",
    "# Initialize the local database connector\n",
    "local_connector = DatabaseConnector(creds_file=\"local_db_creds.yaml\")\n",
    "\n",
    "# Upload the cleaned data to the local database\n",
    "local_connector.upload_to_db(cleaned_orders_df, \"orders_table\")\n",
    "print(\"Uploaded cleaned orders data to the local sales_data database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date times data extracted successfully.\n",
      "Rows after cleaning: 120123\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 120123 entries, 0 to 120160\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   timestamp    120123 non-null  object\n",
      " 1   month        120123 non-null  int64 \n",
      " 2   year         120123 non-null  int64 \n",
      " 3   day          120123 non-null  int64 \n",
      " 4   time_period  120123 non-null  object\n",
      " 5   date_uuid    120123 non-null  object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 6.4+ MB\n",
      "None\n",
      "Uploaded cleaned date times data to the local sales_data database.\n"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtractor\n",
    "from data_cleaning import DataCleaning\n",
    "\n",
    "# Step 1: Extract JSON data from S3\n",
    "data_extractor = DataExtractor()\n",
    "s3_url = \"https://data-handling-public.s3.eu-west-1.amazonaws.com/date_details.json\"\n",
    "date_times_df = data_extractor.extract_json_from_s3(s3_url)\n",
    "\n",
    "if not date_times_df.empty:\n",
    "\n",
    "    # Step 2: Clean the date times data\n",
    "    data_cleaner = DataCleaning()\n",
    "    cleaned_date_times_df = data_cleaner.clean_date_times_data(date_times_df)\n",
    "    print(date_times_df.info())\n",
    "\n",
    "    # Step 3: Upload cleaned data to the local database\n",
    "    local_connector = DatabaseConnector(creds_file=\"local_db_creds.yaml\")\n",
    "    local_connector.upload_to_db(cleaned_date_times_df, \"dim_date_times\")\n",
    "    print(\"Uploaded cleaned date times data to the local sales_data database.\")\n",
    "else:\n",
    "    print(\"Failed to extract date times data from S3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
